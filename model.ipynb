{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from hungarian import Hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIE(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a channel-independent update rule to node and edge features.\n",
    "    \"\"\"\n",
    "    def __init__(self, d, m):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.W0 = nn.Parameter(torch.empty(size=(d, d)), requires_grad=True)\n",
    "        self.W1 = nn.Parameter(torch.empty(size=(d, m)), requires_grad=True)\n",
    "        self.W2 = nn.Parameter(torch.empty(size=(d, d)), requires_grad=True)\n",
    "        self.W3 = nn.Parameter(torch.empty(size=(m, m)), requires_grad=True)\n",
    "        self.W4 = nn.Parameter(torch.empty(size=(d, m)), requires_grad=True)\n",
    "       \n",
    "        nn.init.xavier_normal_(self.W0)\n",
    "        nn.init.xavier_normal_(self.W1)\n",
    "        nn.init.xavier_normal_(self.W2)\n",
    "        nn.init.xavier_normal_(self.W3)\n",
    "   \n",
    "    def forward(self, H, A, E):\n",
    "        '''\n",
    "       H - bs * k * d\n",
    "       A - bs * k * k\n",
    "       E - bs * m * k * k\n",
    "       '''\n",
    "        batch_size, k, d = H.shape\n",
    "        m = E.shape[1]\n",
    " \n",
    "        W1E = E.reshape(batch_size, m, k*k).permute(0, 2, 1) @ self.W1.T  # tensor of shape (batch_size, k*k, d)\n",
    "        W1E = W1E.permute(0, 2, 1).reshape(batch_size, d, k, k)  # tensor of shape (batch_size, d, k, k)\n",
    "        W2H = H @ self.W2\n",
    "        W0H = H @ self.W0\n",
    "        W3E = (E.permute(0,2,3,1) @ self.W3).permute(0,3,1,2)\n",
    " \n",
    "        W4H = H @ self.W4\n",
    " \n",
    "        W1E_W2H = (A.view(bs,1,k,k) * W1E) @ W2H.permute(0,2,1).view((batch_size,d,k,1))\n",
    "        W1E_W2H = W1E_W2H.reshape((batch_size,d,k)).permute(0,2,1)\n",
    "       \n",
    "        H_out = torch.relu(W1E_W2H) + torch.relu(W0H)\n",
    "       \n",
    "        h = W4H.permute(0,2,1).view(bs,m,k,1) - W4H.permute(0,2,1).view(bs,m,1,k)\n",
    "        hE = torch.abs(h) * E\n",
    " \n",
    "        E_out = torch.relu(hE) + torch.relu(W3E)\n",
    " \n",
    "        return H_out, E_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityMatrix(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a similarity matrix between two graphs based on the node features.\n",
    "    \"\"\"\n",
    "    def __init__(self, d, tau=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        arr = torch.empty((d, ))\n",
    "        arr.uniform_()\n",
    "        \n",
    "        self.L = nn.Parameter(torch.diag(arr), requires_grad=True)\n",
    "        self.tau = tau\n",
    "    \n",
    "    def forward(self, H1, H2):\n",
    "        \n",
    "        return torch.exp(H1 @ self.L @ H2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sinkhorn(nn.Module):\n",
    "    \"\"\"\n",
    "    Brings a matrix to a doubly-stochastic form via Sinkhorn algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_iter=20):\n",
    "        super().__init__()\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def forward(self, M):\n",
    "        bs, k, _ = M.shape\n",
    "        one = torch.ones((k,1))\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            norm_row = M @ one @ one.T\n",
    "            M = M / norm_row\n",
    "\n",
    "            M = M / (one @ one.T @ M)\n",
    "        \n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossGraphMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Performs feature merging between two graphs.\n",
    "    \n",
    "    input: H1, H2 - node features bs * k * d\n",
    "           S - similarity matrix of bs * k * k\n",
    "           \n",
    "    output: H1_out, H2_out - merged node features \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = nn.Parameter(torch.empty(size=(2*d, d)), requires_grad=True)\n",
    "    \n",
    "    def forward(self, H1, H2, S):\n",
    "        H1_out = torch.cat([H1, S@H2]) @ self.W\n",
    "        H2_out = torch.cat([H2, S.T@H1]) @ self.W\n",
    "        \n",
    "        return H1_out, H2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hung_attention(S, S_true):\n",
    "    '''\n",
    "    input: S - matching matrix bs * k * k\n",
    "           S_true - true matching matrix bs * k * k\n",
    "           \n",
    "    output: Z - hungarian attention mask bs*k*k which should be elementwise\n",
    "                multiplied by needed loss function\n",
    "    '''\n",
    "    \n",
    "    bs = S.shape[0]\n",
    "    Z = torch.zeros_like(S, requires_grad=False)\n",
    "    \n",
    "    for i in range(bs):\n",
    "        hungarian = Hungarian(S[i], is_profit_matrix=True)\n",
    "        hungarian.calculate()\n",
    "\n",
    "        idx = torch.tensor(hungarian.get_results())\n",
    "\n",
    "        Z_buf = torch.zeros(S.shape, requires_grad=False)\n",
    "        Z_buf[idx[:,0],idx[:,1]] = 1.\n",
    "\n",
    "        Z_buf = Z_buf.long() | S_true.long()\n",
    "        \n",
    "        Z[i] = Z_buf.double()\n",
    "    \n",
    "    return Z.double()\n",
    "\n",
    "\n",
    "class HungarianLoss(nn.Module):\n",
    "    def __init__(self, hung_attention=False):\n",
    "        super().__init__()\n",
    "        self.hung_attention = hung_attention\n",
    "        \n",
    "    def forward(self, S, S_true):\n",
    "        if self.hung_attention:\n",
    "            Z = hung_attention(S, S_true)\n",
    "        else:\n",
    "            Z = torch.ones(S.shape)\n",
    "        \n",
    "        loss = -torch.sum(Z * (S_true * torch.log(S) + (1. - S_true) * torch.log(1. - S)))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HungarianModel(nn.Module):\n",
    "    def __init(self, d, m):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cie1 = CIE(d, m)\n",
    "        self.cie2 = CIE(d, m)\n",
    "        self.sim = SimilarityMatrix(d)\n",
    "        self.cross = CrossGraphMerging(d)\n",
    "        self.sinkhorn = Sinkhorn(n_iter=10)\n",
    "        \n",
    "    def forward(self, H1, E1, H2, E2):\n",
    "        \n",
    "        H1, E1 = self.cie1(H1, E1)\n",
    "        H2, E2 = self.cie1(H2, E2)\n",
    "        S = self.sinkhorn(self.sim(H1, H2))\n",
    "        H1, H2 = self.cross(H1, H2, S)\n",
    "        H1, E1 = self.cie2(H1, E1)\n",
    "        H2, E2 = self.cie2(H2, E2)\n",
    "        S = self.sinkhorn(self.sim(H1, H2))\n",
    "        \n",
    "        return S"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
