{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from hungarian import Hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIE(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a channel-independent update rule to node and edge features.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, m_in, d_out, m_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.m_out = m_out\n",
    "        self.W0 = nn.Parameter(torch.empty(size=(d_in, d_out)), requires_grad=True)\n",
    "        self.W1 = nn.Parameter(torch.empty(size=(m_in, d_out)), requires_grad=True)\n",
    "        self.W2 = nn.Parameter(torch.empty(size=(d_in, d_out)), requires_grad=True)\n",
    "        self.W3 = nn.Parameter(torch.empty(size=(m_in, m_out)), requires_grad=True)\n",
    "        self.W4 = nn.Parameter(torch.empty(size=(d_in, m_out)), requires_grad=True)\n",
    "        self.W5 = nn.Parameter(torch.empty(size=(m_in, m_out)), requires_grad=True)\n",
    "       \n",
    "        nn.init.xavier_normal_(self.W0)\n",
    "        nn.init.xavier_normal_(self.W1)\n",
    "        nn.init.xavier_normal_(self.W2)\n",
    "        nn.init.xavier_normal_(self.W3)\n",
    "        nn.init.xavier_normal_(self.W4)\n",
    "        nn.init.xavier_normal_(self.W5)\n",
    "   \n",
    "    def forward(self, H, E, verbose=False):\n",
    "        '''\n",
    "       H - bs * k * d\n",
    "       A - bs * k * k\n",
    "       E - bs * m * k * k\n",
    "       '''\n",
    "        batch_size, k, d_in = H.shape\n",
    "        m_in = E.shape[1]\n",
    "        d_out, m_out = self.d_out, self.m_out\n",
    "        \n",
    "        W1E = E.reshape(batch_size, m_in, k*k).permute(0, 2, 1) @ self.W1  # tensor of shape (batch_size, k*k, d_out)\n",
    "\n",
    "        W1E = W1E.permute(0, 2, 1).reshape(batch_size, d_out, k, k)  # tensor of shape (batch_size, d, k, k)\n",
    "\n",
    "        W2H = H @ self.W2\n",
    "\n",
    "        W0H = H @ self.W0\n",
    "\n",
    "        W3E = (E.permute(0,2,3,1) @ self.W3).permute(0,3,1,2)\n",
    "\n",
    " \n",
    "        W4H = H @ self.W4\n",
    "\n",
    " \n",
    "        W1E_W2H =  W1E @ W2H.permute(0,2,1).view((batch_size, d_out, k, 1))\n",
    "\n",
    "        W1E_W2H = W1E_W2H.reshape((batch_size, d_out, k)).permute(0,2,1)\n",
    "\n",
    "        \n",
    "       \n",
    "        H_out = torch.relu(W1E_W2H) + torch.relu(W0H)\n",
    "        \n",
    "        W5E = E.reshape(batch_size, m_in, k*k).permute(0, 2, 1) @ self.W5  # tensor of shape (batch_size, k*k, d_out)\n",
    "\n",
    "        W5E = W5E.permute(0, 2, 1).reshape(batch_size, m_out, k, k)  # tensor of shape (batch_size, d, k, k)\n",
    "       \n",
    "        h = W4H.permute(0,2,1).reshape(batch_size, m_out, k, 1) - W4H.permute(0, 2, 1).reshape(batch_size, m_out, 1, k)\n",
    "        \n",
    "        hE = torch.abs(h) * W5E\n",
    "\n",
    " \n",
    "        E_out = torch.relu(hE) + torch.relu(W3E)\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print('E: ', E)\n",
    "            print('H: ', H)\n",
    "            print('W0: ', self.W0)\n",
    "            print('W1: ', self.W1)\n",
    "            print('W2: ', self.W2)\n",
    "            print('W3: ', self.W3)\n",
    "            print('W4: ', self.W4)\n",
    "            print('W1E: ', W1E)\n",
    "            print('W1E: ', W1E)\n",
    "            print('W2H: ', W2H)\n",
    "            print('W0H: ', W0H)\n",
    "            print('W3E: ', W3E)\n",
    "            print('W4H: ', W4H)\n",
    "            print('W1E_W2H: ', W1E_W2H)\n",
    "            print('W1E_W2H: ', W1E_W2H)\n",
    "            print('H_out: ', H_out)\n",
    "            print('h: ', h)\n",
    "            print('hE: ', hE)\n",
    "            print('E_out: ', E_out)\n",
    " \n",
    "        return H_out, E_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityMatrix(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a similarity matrix between two graphs based on the node features.\n",
    "    \"\"\"\n",
    "    def __init__(self, d, tau=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        arr = torch.empty((d, ))\n",
    "        arr.uniform_()\n",
    "        arr = arr\n",
    "        self.L = nn.Parameter(torch.diag(arr), requires_grad=True)\n",
    "        self.tau = tau\n",
    "    \n",
    "    def forward(self, H1, H2):\n",
    "        exp = self.tau * H1 @ self.L @ H2.transpose(1, 2)\n",
    "        max_elements, _ = torch.max(exp, dim=-1, keepdims=True)\n",
    "        max_elements, _ = torch.max(max_elements, dim=-1, keepdims=True)\n",
    "        return torch.exp(exp - max_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sinkhorn(nn.Module):\n",
    "    \"\"\"\n",
    "    Brings a matrix to a doubly-stochastic form via Sinkhorn algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_iter=20):\n",
    "        super().__init__()\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def forward(self, M):\n",
    "        bs, k, _ = M.shape\n",
    "        ones = torch.ones((k,k))\n",
    "        \n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            M = M - (M @ ones + ones @ M + ones) / k + ones @ M @ ones / k ** 2\n",
    "        \n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossGraphMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Performs feature merging between two graphs.\n",
    "    \n",
    "    input: H1, H2 - node features bs * k * d\n",
    "           S - similarity matrix of bs * k * k\n",
    "           \n",
    "    output: H1_out, H2_out - merged node features \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = nn.Parameter(torch.empty(size=(2*d, d)), requires_grad=True)\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "    \n",
    "    def forward(self, H1, H2, S):\n",
    "        H1_out = torch.cat([H1, S@H2], dim=-1) @ self.W\n",
    "        H2_out = torch.cat([H2, S.transpose(-1, -2)@H1], dim=-1) @ self.W\n",
    "        \n",
    "        return H1_out, H2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hung_attention(S, S_true):\n",
    "    '''\n",
    "    input: S - matching matrix bs*k*k\n",
    "           S_true - true matching matrix bs*k*k\n",
    "           \n",
    "    output: Z - hangarian attention mask bs*k*k which should be elementwise\n",
    "                multiplied by needed loss function\n",
    "    '''\n",
    "    \n",
    "    #assert S_true.sum() == S_true.shape[0]\n",
    "    bs, k, _ = S.shape\n",
    "    Z = torch.zeros_like(S, requires_grad=False)\n",
    "    \n",
    "    for i in range(bs):\n",
    "        hungarian = Hungarian(S[i,:,:].detach(), is_profit_matrix=True)\n",
    "        hungarian.calculate()\n",
    "\n",
    "        idx = torch.tensor(hungarian.get_results())\n",
    "\n",
    "        Z_buf = torch.zeros((k,k), requires_grad=False)\n",
    "        Z_buf[idx[:,0],idx[:,1]] = 1.\n",
    "\n",
    "        Z_buf = Z_buf.long() | S_true[i,:,:].long()\n",
    "        \n",
    "        Z[i,:,:] = Z_buf.double()\n",
    "    \n",
    "    return Z\n",
    "\n",
    "\n",
    "class PermutationLoss(nn.Module):\n",
    "    def __init__(self, hung_attention=False):\n",
    "        super().__init__()\n",
    "        self.hung_attention = hung_attention\n",
    "        \n",
    "    def forward(self, S, S_true):\n",
    "        if self.hung_attention:\n",
    "            Z = hung_attention(S, S_true)\n",
    "        else:\n",
    "            Z = torch.ones(S.shape)\n",
    "        \n",
    "        loss = -torch.sum(Z * (S_true * torch.log(S) + (1. - S_true) * torch.log(1. - S)))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HungarianModel(nn.Module):\n",
    "    def __init__(self, d_in, m_in, d_out, m_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cie1 = CIE(d_in, m_in, d_out, m_out)\n",
    "        self.cie2 = CIE(d_out, m_out, d_out, m_out)\n",
    "        self.sim = SimilarityMatrix(d_out)\n",
    "        self.cross = CrossGraphMerging(d_out)\n",
    "        self.sinkhorn = Sinkhorn(n_iter=10)\n",
    "        \n",
    "    def forward(self, H1, E1, H2, E2):\n",
    "        \n",
    "        H1, E1 = self.cie1(H1, E1)\n",
    "        H2, E2 = self.cie1(H2, E2)\n",
    "#         print(f'\\n\\nafter cie1 \\n\\nH1: {H1} \\n\\n\\nE1: {E1}')\n",
    "        S = self.sinkhorn(self.sim(H1, H2))\n",
    "#         print(f'\\n\\nafter sink1 \\n\\nS: {S}')\n",
    "        H1, H2 = self.cross(H1, H2, S)\n",
    "#         print(f'\\n\\nafter cross \\n\\nH1: {H1}')\n",
    "        H1, E1 = self.cie2(H1, E1)\n",
    "#         print(f'\\n\\nafter cie2 \\n\\nH1: {H1} \\n\\n\\nE1: {E1}')\n",
    "        H2, E2 = self.cie2(H2, E2)\n",
    "#         print(f'\\n\\sim \\n\\nS: {self.sim(H1, H2).sum()}')\n",
    "        S = self.sinkhorn(self.sim(H1, H2))\n",
    "#         print(f'\\n\\nafter sink2 \\n\\nS: {S}')\n",
    "        \n",
    "        return S"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
